{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Data analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from utils.config import tag2pos_table, level_table\n",
    "from utils.EGP import Egp\n",
    "from utils.EVP import Evp\n",
    "from utils.BNC import Bnc\n",
    "import kenlm\n",
    "\n",
    "model = kenlm.Model('/home/nlplab/jjc/gec/lm/coca.prune.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm(last_sent, ngram):\n",
    "    sentence = last_sent + ' ' + ngram\n",
    "    score = model.score(sentence, bos=True, eos=False) # / len(sentence.split())\n",
    "    return score\n",
    "\n",
    "\n",
    "def level_score(ngram):\n",
    "    levels = [Evp.get_level(token) for token in ngram.split()]\n",
    "    score = sum([level_table[level] if level else 0 for level in levels]) / len(levels)\n",
    "    return score\n",
    "\n",
    "\n",
    "def normalize_tag(tag):\n",
    "    return tag2pos_table[tag]+'.' if tag in tag2pos_table else tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_patterns(related_patterns, text_sent, headword, pos, top_k=1):\n",
    "    # not containing related ngram patterns\n",
    "    related_patterns = filter(lambda ptn: ptn[1] not in [9, 12] and headword in Bnc.ngram_groups[ptn], related_patterns)\n",
    "\n",
    "    # group patterns by number\n",
    "    pattern_groups = defaultdict(list)\n",
    "    for pattern, no in related_patterns:\n",
    "        pattern_groups[no].append(pattern)\n",
    "        \n",
    "    # calc LM and get top k\n",
    "    targets = []\n",
    "    for no in pattern_groups:\n",
    "        candidates = [ng for pattern in pattern_groups[no] for ng in Bnc.ngram_groups[(pattern, no)][headword]]\n",
    "        candidates = filter(lambda ng: len(ng.split()) < 7, candidates)\n",
    "        candidates = filter(lambda ng: len(Bnc.sentences[ng]) > 1, candidates)\n",
    "        scores = [(ng, level_score(ng), lm(text_sent, ng)) for ng in candidates]\n",
    "        scores = sorted(scores, key=lambda x: x[2], reverse=True)[:10]\n",
    "\n",
    "        if len(scores) > 0:\n",
    "            scores = sorted(scores, key=lambda x: x[1], reverse=True)[:5]\n",
    "            ngrams = [score[0] for score in scores]\n",
    "            lm_score = sum(score[2] for score in scores) / len(scores)\n",
    "            targets.append({'no': no, 'level': Egp.get_level(no), 'pos': normalize_tag(pos), \n",
    "                            'lm': lm_score, 'pattern': Egp.get_norm_pattern(no), 'ngrams': ngrams,\n",
    "                            'category': Egp.get_category(no), 'subcategory': Egp.get_subcategory(no),\n",
    "                            'statement': Egp.get_statement(no) } )\n",
    "    \n",
    "    # sort by lm\n",
    "    targets = sorted(targets, key=lambda t: t['lm'], reverse=True)\n",
    "    \n",
    "    return targets\n",
    "\n",
    "\n",
    "def suggest_sentences(ngram):\n",
    "    return Bnc.sentences[ngram][:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.grammar import iterate_all_patterns\n",
    "\n",
    "def auto_suggest(parse_sent, gets):\n",
    "    last_word, last_index = parse_sent[-1], len(parse_sent) - 1\n",
    "    if last_word.is_punct and len(parse_sent) > 1:\n",
    "        last_word, last_index = parse_sent[-2], len(parse_sent) - 2\n",
    "        \n",
    "    headword, pos = last_word.text, last_word.tag_\n",
    "    text_sent = parse_sent.text.rsplit(' ', maxsplit=1)[0] # to text\n",
    "    \n",
    "    # get max get\n",
    "    gets = [get for get in gets if last_index in get['indices']]\n",
    "    get = None\n",
    "    \n",
    "    related_patterns = Bnc.pattern_groups[pos].union(Bnc.pattern_groups[headword])\n",
    "    if len(gets) > 0:\n",
    "        get = max(gets, key=lambda get: level_table[get['level']])\n",
    "        get['pattern'] = Egp.get_norm_pattern(get['no'])\n",
    "        related_patterns = filter(lambda ptn: level_table[Egp.get_level(ptn[1])] > level_table[get['level']] and get['no'] != ptn[1], related_patterns)\n",
    "    \n",
    "    patterns = suggest_patterns(related_patterns, text_sent, headword, pos)\n",
    "    \n",
    "    return get, { 'patterns': patterns, 'collocations': [] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('freindly', None), ('Friendly', None), ('unfriendly', 'B1'), ('friendlier', None), ('friendliest', None), ('hospitable', 'C1'), ('anybody_Petney', None), ('friendliness', 'B2'), ('congenial', None), ('detecting_gastrointestinal_disorders', None), ('Petney_observed', None), ('sociable', 'B1'), ('Jowzjan_province_Darzab_district', None), ('easygoing', 'B1'), ('oriented', None), ('hourslong_confrontation', None), ('welcoming', None), ('pleasant', 'A2'), ('staff_hrogers', None), ('approachable', None), ('unintimidating', None), ('friendy', None), ('WTOP_strives', None), ('Maya_Derkovic', None), ('courteous', 'C2'), ('cordial', None), ('accommodating', None), ('cheerful', 'B1'), ('Luttrell_Auction', None), ('convivial', None), ('orientated', None), ('www.playthq.com', None), ('amiable', None), ('lively', 'B1'), ('loving', None), ('chatty', 'C1'), ('unstuffy', None), ('personable', None), ('warm', 'A1'), ('respectful', 'C1'), ('neighborly', None), ('hireable', None), ('considerate', 'C1'), ('Metal_Skool_Ã‚', None), ('decidedly_unfriendly', None), ('jovial', None), ('Coach_Bowden_Spetman', None), ('hostile', 'C1'), ('polite', 'A2'), ('relaxed', 'B1'), ('##th_####Vicious_Cycle', None), ('consious', None), ('ophthalmological_indications', None), ('familylike', None), ('cuddly', None), ('Handsome_cultured', None), ('unpretentious_Toyoda', None), ('gracious', None), ('Congenial', None), ('Nom_chat', None), ('Donna_Diegel_pet', None), ('good_natured', None), ('homely', None), ('environ_mentally', None), ('unfailingly_pleasant', None), ('visit_RadiologyInfo.org', None), ('Mark_R._Prausnitz', None), ('gregarious', None), ('good_humored', None), ('talkative', 'C1'), ('unthreatening', None), ('Donna_Diegel_Meet', None), ('soups_pasta_sauces', None), ('chummy', None), ('wittiness_Squire', None), ('wholesome', None), ('supportive', 'C1'), ('ON_YOUR_PHONE', None), ('GelTech_Solutions_creates', None), ('laidback', None), ('cheery', None), ('warm_hearted', None), ('tolerant', 'C1'), ('parades_uptown', None), ('centric', None), ('knowlegable', None), ('Ruimsig_Stadium', None), ('cozy', 'B1'), ('####Vicious_Cycle_Portland', None), ('likeable', None), ('baking_lasagna', None), ('Sharon_Watterson_Eco', None), ('stand_offish', None), ('warmly', 'B2'), ('sandwiches_entrees', None), ('agreeable', None), ('affectionate', 'C2'), ('Margaret_Bernstorff', None), ('Project_Bootheel', None), ('Sardo_Grill_&', None)]\n",
      "[{'vocab': 'courteous', 'level': 'C2'}, {'vocab': 'hospitable', 'level': 'C1'}, {'vocab': 'chatty', 'level': 'C1'}, {'vocab': 'respectful', 'level': 'C1'}, {'vocab': 'considerate', 'level': 'C1'}, {'vocab': 'unfriendly', 'level': 'B1'}, {'vocab': 'sociable', 'level': 'B1'}, {'vocab': 'easygoing', 'level': 'B1'}, {'vocab': 'cheerful', 'level': 'B1'}, {'vocab': 'lively', 'level': 'B1'}]\n"
     ]
    }
   ],
   "source": [
    "from utils.parser import nlp\n",
    "from utils.grammar import iterate_all_patterns\n",
    "\n",
    "parse = nlp('Those students are friendly')\n",
    "get, sugs = auto_suggest(parse, iterate_all_patterns(parse))\n",
    "from pprint import pprint\n",
    "# pprint(sugs)\n",
    "from utils.vocabulary import recommend_vocabs\n",
    "print(recommend_vocabs('friendly'))\n",
    "# for sug in sugs['patterns']:\n",
    "#     if sug['level'].startswith('A'): continue\n",
    "#     print(sug['level'], sug['ngram'], str(round(sug['lm'],2)) + ' \\\\\\\\', sep=' & ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8098545\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('recommend.prune.pickle', 'rb') as handle:\n",
    "    info = pickle.load(handle)\n",
    "    counts, ngrams, sentences = info['counts'], info['ngrams'], info['sentences']\n",
    "            \n",
    "# total = 0\n",
    "# for item, count in counts.items():\n",
    "#     total += count\n",
    "# print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def auto_suggest(headword, pos, last_sent):\n",
    "#     related_patterns = pattern_groups[headword] + pattern_groups[pos] # get headword matched POS and first word\n",
    "#     related_patterns = [related_pat for related_pat in related_patterns if headword in ngram_groups[related_pat]]\n",
    "\n",
    "#     # merge same number rule\n",
    "#     total = Counter()\n",
    "#     for pattern, no in related_patterns: \n",
    "#         for ngram in ngram_groups[(pattern, no)][headword]:\n",
    "#             total[no] += ngrams[(pattern, no)][ngram]\n",
    "#     top_k_keys = dict(total.most_common(3))\n",
    "\n",
    "#     # get top k patterns\n",
    "#     target_patterns = filter(lambda key: key[1] in top_k_keys, related_patterns)\n",
    "\n",
    "#     # normalize patterns\n",
    "#     target_norm_patterns = defaultdict(list)\n",
    "#     for pattern, no in target_patterns:\n",
    "#         target_norm_patterns[normalize_pattern(headword, pattern)].append((pattern, no))\n",
    "        \n",
    "#     # retrieve ngrams example\n",
    "#     total = 0\n",
    "#     target_ngrams = []\n",
    "#     for norm_pattern in target_norm_patterns: # (pattern, no)\n",
    "#         scores = [lm(last_sent, ng) for pattern, no in target_norm_patterns[norm_pattern] \n",
    "#                   for ng in ngram_groups[(pattern, no)][headword]] # get ngram in given patterns\n",
    "#         scores = sorted(scores, key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "#         t_ngrams = [ng[0] for ng in scores]\n",
    "#         avg = 1 / abs(sum([s[1] for s in scores]) / len(scores))\n",
    "#         total += avg        \n",
    "\n",
    "#         target_ngrams.append({\n",
    "#             'pattern': norm_pattern, 'pos': normalize_tag(pos),\n",
    "#             'no': no, 'level': Egp.get_level(no), \n",
    "#             # 'count': counts[(pattern, no)], \n",
    "#             'lm': avg,\n",
    "#             'category': Egp.get_category(no), 'subcategory': Egp.get_subcategory(no),\n",
    "#             'ngrams': t_ngrams, \n",
    "#             'sentence': sentences[t_ngrams[0]][0] })\n",
    "\n",
    "#     # get means and sorted\n",
    "#     for ng in target_ngrams: \n",
    "#         ng['lm'] = ng['lm'] / total\n",
    "        \n",
    "#     return target_ngrams\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
