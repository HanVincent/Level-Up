{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    import re\n",
    "\n",
    "    infix_re = compile_prefix_regex(nlp.Defaults.infixes)\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "\n",
    "from utils.grammar import generate_candidates, iterate_all_patterns, remove_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \n",
    "    def __init__(self, token, doc):\n",
    "        index, text, lemma, norm, pos, tag, dep, head, children = token.split('|')\n",
    "        children = children.strip()\n",
    "\n",
    "        self.i = int(index)\n",
    "        self.text = text\n",
    "        self.norm_ = norm\n",
    "        self.lemma_ = lemma\n",
    "        self.dep_ = dep\n",
    "        self.pos_ = pos\n",
    "        self.tag_ = tag\n",
    "        self.head_ = int(head)\n",
    "        self.children_ = children\n",
    "        self.doc = doc\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if name == 'head':\n",
    "            return self.doc[head_]\n",
    "        elif name == 'children':\n",
    "            return [self.doc[int(e)] for e in self.children_.split(',')] if self.children_ else []\n",
    "        else:\n",
    "            return self[name]\n",
    "    \n",
    "    \n",
    "class Parse(list):\n",
    "\n",
    "    def __init__(self, entry):\n",
    "        tokens = [token for token in entry.strip().split('\\t') if token]\n",
    "        text, tokens = tokens[0], tokens[1:]\n",
    "        \n",
    "        self.extend([DotDict(token, self) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gzip.open('bnc.parse.txt.gz', 'rt', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import json\n",
    "\n",
    "def duplicate_sent(sent):\n",
    "    sent = sent.replace('\\\\', '')\n",
    "    tokens = []\n",
    "    for token in sent.split():\n",
    "        tokens.append(token.split('/') if '/' in token else [token])\n",
    "\n",
    "    composes = product(*tokens)\n",
    "    sents = [' '.join(compose) for compose in composes]\n",
    "    \n",
    "    return sents\n",
    "\n",
    "def find_values(id, json_repr):\n",
    "    results = []\n",
    "\n",
    "    def _decode_dict(a_dict):\n",
    "        try: results.append(a_dict[id])\n",
    "        except KeyError: pass\n",
    "        return a_dict\n",
    "\n",
    "    json.loads(json_repr, object_hook=_decode_dict)  # Return value ignored.\n",
    "    results = [s for result in results for sent in result for s in duplicate_sent(sent) ] # flatten\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camb_dict = json.dumps(json.load(open('data/cambridge.dict.json')))\n",
    "# camb_ph_dict = json.dumps(json.load(open('data/cambridge.dict.phrase.json')))\n",
    "# dict_sentences = find_values('dic_examples', camb_dict) + find_values('dic_examples', camb_ph_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "ngrams = defaultdict(Counter)\n",
    "counts = Counter()\n",
    "sentences = defaultdict(list)\n",
    "\n",
    "# for i, entry in enumerate(fs):\n",
    "for i, sentence in enumerate(dict_sentences):\n",
    "    # parse = Parse(entry)\n",
    "    parse = nlp(sentence, disable=['ner'])\n",
    "\n",
    "    # 1. generate possible sentences\n",
    "    parses = generate_candidates(parse)\n",
    "\n",
    "    # 2. find patterns for each candidate\n",
    "    gets = [get for parse in parses for get in iterate_all_patterns(parse)]\n",
    "\n",
    "    # 3. remove duplicate\n",
    "    gets = remove_overlap(parse, gets)\n",
    "\n",
    "    text = ' '.join([tk.text for tk in parse])\n",
    "    for get in gets:\n",
    "        counts[(get['match'], get['no'])] += 1\n",
    "        ngrams[(get['match'], get['no'])][get['ngram']] += 1\n",
    "        sentences[get['ngram']].append(text)\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(i)\n",
    "        with open('new.recommend.pickle', 'wb') as handle:\n",
    "            pickle.dump({ 'counts': counts, 'ngrams': ngrams, 'sentences': sentences }, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new.recommend.pickle', 'wb') as handle:\n",
    "    pickle.dump({ 'counts': counts, 'ngrams': ngrams, 'sentences': sentences }, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
