{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Cambridge Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "\n",
    "import requests\n",
    "import re, json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'englishprofile'\n",
    "PASSWORD = 'vocabulary'\n",
    "\n",
    "base_url = \"http://vocabulary.englishprofile.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_i_span = re.compile('<span class=\"i\">(.*?)</span>')\n",
    "re_level_cls = re.compile(r\"freq-[ABC][12]\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(endpoint):\n",
    "    return requests.get(base_url + endpoint, auth=(USERNAME, PASSWORD))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrefs = []\n",
    "# indices = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# for index in indices:\n",
    "#     endpoint = \"/dictionary/word-list/us/a1_c2/\" + index\n",
    "    \n",
    "#     response = crawl(endpoint)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     ul = soup.find('div',attrs={'id':'groupResult'}).find('ul')\n",
    "#     lis = ul.find_all('li')\n",
    "    \n",
    "#     print(len(lis))\n",
    "\n",
    "#     hrefs.extend([li.find('a')['href'] for li in lis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_len = len(hrefs)\n",
    "\n",
    "# ws = open('dict.slim.txt', 'w', encoding='utf8')\n",
    "# for idx, href in enumerate(hrefs):\n",
    "#     print(\"{} / {}\".format(idx, total_len))\n",
    "    \n",
    "#     response = crawl(href)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     ul = soup.find('div',attrs={'id':'result'}).find('ul')\n",
    "#     lis = ul.find_all('li')\n",
    "    \n",
    "#     for li in lis:\n",
    "#         a_href = li.find('a')['href']\n",
    "        \n",
    "#         main_span = li.find('span', {'class': 'display'})\n",
    "        \n",
    "#         vocab = main_span.find('span', {'class': 'base'}).decode_contents()\n",
    "#         vocab = re_i_span.sub(r'<i>\\1</i>', vocab)\n",
    "        \n",
    "#         pos = main_span.find_all('span', {'class': 'pos'})\n",
    "#         pos = ','.join([p.text for p in pos])\n",
    "        \n",
    "#         gw  = main_span.find('span', {'class': 'gw'})\n",
    "#         gw  = gw.text if gw else \"\"\n",
    "        \n",
    "#         level = main_span.find('span', {'class': re_level_cls})\n",
    "#         level = level.text if level else \"\"\n",
    "        \n",
    "#         entry = {\n",
    "#             'href': a_href,\n",
    "#             'vocab': vocab,\n",
    "#             'pos': pos,\n",
    "#             'gw': gw,\n",
    "#             'level': level\n",
    "#         }\n",
    "        \n",
    "#         print(entry['vocab'], entry['level'], entry['pos'], entry['gw'], entry['href'], sep='\\t', file=ws)\n",
    "        \n",
    "# ws.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Detailed Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gw(gw_block):\n",
    "    sense_div = gw_block.find('div', attrs={'class': 'sense'}) \n",
    "        \n",
    "    # get phrase information block\n",
    "    if gw_block.find('div', attrs={'class': 'phraserec'}):\n",
    "        is_phrase = True\n",
    "        phrase = gw_block.find('h3', attrs={'class': 'phrase'}).decode_contents()\n",
    "        phrase = re_i_span.sub(r'<i>\\1</i>', phrase)\n",
    "\n",
    "    # get word definition block\n",
    "    else:\n",
    "        is_phrase = False\n",
    "        gw = gw_block.find('h3', attrs={'class': 'gw'})\n",
    "        gw = gw.text.strip() if gw else \"\"\n",
    "\n",
    "        gram = sense_div.find_all('abbr', {'class': 'gram'})\n",
    "        gram = ' '.join([g.text for g in gram])\n",
    "\n",
    "    level = sense_div.find('span', {'class': re_level_cls})\n",
    "    level = level.text if level else \"\"   \n",
    "\n",
    "    definition = sense_div.find('span', attrs={'class': 'def'}).text.strip()\n",
    "    dic_div = sense_div.find('div', attrs={'class': 'examp-block'})\n",
    "    if dic_div:\n",
    "        dic_examples = [ex.text for ex in dic_div.find_all('blockquote', attrs={'class': 'examp'})]\n",
    "    else:\n",
    "        dic_examples = []\n",
    "    \n",
    "    clc_block = sense_div.find('blockquote', attrs={'class': 'clc'})\n",
    "    if clc_block:\n",
    "        clc_block.find('div', attrs={'class': 'clc_before'}).clear()\n",
    "        src_div = clc_block.find('div', attrs={'class': 'src'})\n",
    "        if src_div:\n",
    "            src = src_div.text.strip()\n",
    "            src_div.clear()\n",
    "        else:\n",
    "            src = \"NONE\"\n",
    "            \n",
    "        clc = clc_block.text.strip() + ' (' + src + ')'\n",
    "    else:\n",
    "        clc = \"NONE (NONE)\"\n",
    "        \n",
    "\n",
    "    if is_phrase:\n",
    "        return {'phrase': phrase, 'is_phrase': is_phrase, 'level': level, \n",
    "                'definition': definition, 'dic_examples': dic_examples,\n",
    "                'clc': clc}\n",
    "    else:\n",
    "        return {'gw': gw, 'gram': gram, 'is_phrase': is_phrase, 'level': level, \n",
    "                'definition': definition, 'dic_examples': dic_examples,\n",
    "                'clc': clc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = set()\n",
    "def is_repeated(href):\n",
    "    idx = urlparse(href).path.split('/')[-1]\n",
    "    \n",
    "    if idx in cache:\n",
    "        return True\n",
    "    else:\n",
    "        cache.add(idx)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = defaultdict(lambda: defaultdict(lambda: [])) # word -> pos -> entry\n",
    "phrase_dictionary = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: []))) # word -> pos -> phrase -> entry\n",
    "\n",
    "file = 'dict.slim.txt'\n",
    "for i, line in enumerate(open(file, 'r', encoding='utf8')):\n",
    "    vocab, level, pos, gw, href = line.split('\\t')\n",
    "    # avoid repeated crawling\n",
    "    if is_repeated(href): continue\n",
    "    \n",
    "    try:\n",
    "        print(i, vocab)\n",
    "    except:\n",
    "        print(\"ERROR: \", i)\n",
    "        continue\n",
    "\n",
    "    response = crawl(href)\n",
    "    sleep(0.5) # Time in seconds.\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    main_div = soup.find('div',attrs={'id':'dictionary_entry'}).find('span', attrs={'class': 'entry'})\n",
    "    \n",
    "    hw = main_div.find('div', attrs={'class': 'head'}).find('h1', attrs={'class': 'hw'}).text\n",
    "\n",
    "    pos_blocks = main_div.find_all('div', attrs={'class': 'posblock'})\n",
    "\n",
    "    for pos_block in pos_blocks:\n",
    "        ### Phrase or Word\n",
    "        pos_span = pos_block.find('span', attrs={'class': 'posgram'})\n",
    "        pos_gram = pos_span.text.strip() if pos_span else \"\"\n",
    "\n",
    "        gw_blocks = pos_block.find_all('div', attrs={'class': 'gwblock'})\n",
    "\n",
    "        for gw_block in gw_blocks:\n",
    "            entry = process_gw(gw_block)\n",
    "            dictionary[hw][pos_gram].append(entry)\n",
    "\n",
    "\n",
    "        ### Phrasal Verbs\n",
    "        ph_blocks = pos_block.find_all('div', attrs={'class': 'phrasal_verb'})\n",
    "\n",
    "        for ph_block in ph_blocks:\n",
    "            phrase = ph_block.find('h3', attrs={'class': 'phrase'}).text.strip()\n",
    "\n",
    "            gw_blocks = ph_block.find_all('div', attrs={'class': 'gwblock'})\n",
    "\n",
    "            for gw_block in gw_blocks:\n",
    "                entry = process_gw(gw_block)                \n",
    "                phrase_dictionary[hw][pos_gram][phrase].append(entry)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict.json', 'w', encoding='utf8') as ws:\n",
    "    json.dump(dictionary, ws)\n",
    "    \n",
    "with open('dict.phrase.json', 'w', encoding='utf8') as ws:\n",
    "    json.dump(phrase_dictionary, ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
