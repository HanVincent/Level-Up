{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from math import log\n",
    "from itertools import groupby\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    import re\n",
    "    \n",
    "    infix_re  = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import *\n",
    "from utils.grammar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Dictionary import Dictionary\n",
    "Dict = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used for testing\n",
    "import re\n",
    "re_token = re.compile('\\w+|[,.:;!?]')\n",
    "def is_match(parse, pat):\n",
    "    ### rule to catch\n",
    "    stopwords = re_token.findall(pat.pattern)\n",
    "    norm_tags  = ' '.join([tk.tag_ if tk.norm_ not in stopwords else tk.norm_ for tk in parse])\n",
    "    lemma_tags  = ' '.join([get_lemma(tk, stopwords) for tk in parse])\n",
    "    origin_tags = ' '.join([tk.tag_ if tk.text not in stopwords else tk.text for tk in parse])\n",
    "\n",
    "    return pat.search(norm_tags) or pat.search(lemma_tags) or pat.search(origin_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_vocab(parse):\n",
    "    annotate = [(tk.text, Dict.lookup(tk.lemma_)) for tk in parse]\n",
    "    return annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_profiling(content):\n",
    "    # content = normalize(content)\n",
    "    \n",
    "    sent_profiles = []\n",
    "    for sent in nlp(content).sents:\n",
    "        parse = nlp(normalize(sent.text))\n",
    "        \n",
    "        # 1. find non-overlapped matches\n",
    "        gets = iterate_pats(parse, pat_groups) # match patterns in groups\n",
    "        # print(gets)\n",
    "        # if not gets: continue # non-match\n",
    "        \n",
    "        # 2. recommend related higher pattern in the same group\n",
    "        recs  = recommend_pats(gets, pat_groups)\n",
    "        # print(recs)\n",
    "        \n",
    "        sent_profiles.append({'sent': sent.text, 'parse': ' '.join([tk.text for tk in parse]), \n",
    "                              'gets': gets, 'recs': recs })\n",
    "\n",
    "    return sent_profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_dict  = Egp.get_patterns()\n",
    "sent_dict = Egp.get_examples()\n",
    "\n",
    "### TEMP\n",
    "# delete = [no for no in pat_dict] # no < 1020 or no > 1050\n",
    "# for no in delete: del pat_dict[no]\n",
    "delete = [no for no in sent_dict if no not in pat_dict]\n",
    "for no in delete: del sent_dict[no]\n",
    "###\n",
    "\n",
    "pat_groups = Egp.get_group_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm handsome\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenizer = TreebankWordDetokenizer()\n",
    "detokenizer.detokenize(\"I 'm handsome\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('/atom/corpus/general/BNC/bnc.txt', 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 48.1 s, total: 1min 8s\n",
      "Wall time: 3.46 s\n"
     ]
    }
   ],
   "source": [
    "word_pattern_counter = defaultdict(Counter)\n",
    "\n",
    "for line in lines[:100]:\n",
    "    line = detokenizer.detokenize(line.strip().split(' '))\n",
    "    parse = nlp(normalize(line))\n",
    "\n",
    "    gets = iterate_pats(parse, pat_groups) # match patterns in groups\n",
    "    for get in gets:\n",
    "        pattern = pat_dict[get['no']].pattern\n",
    "        \n",
    "        tokens = get['ngram'].split(' ')\n",
    "        for tk in tokens:\n",
    "            word_pattern_counter[tk][pattern] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'is': Counter({'be JJ': 1}),\n",
       "             'nice': Counter({'be JJ': 1, 'JJ and JJ': 2}),\n",
       "             'and': Counter({'JJ and JJ': 1})})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pattern_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.90222222222222"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4880000 / 100 * 3.46 / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
