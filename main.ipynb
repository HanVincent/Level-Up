{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys, os, re\n",
    "from math import log\n",
    "from itertools import product\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords # might use spacy stopword\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sent):\n",
    "    # strip and replace multiple spaces\n",
    "    return ' '.join(sent.split())\n",
    "\n",
    "\n",
    "def parse_sent(sent):\n",
    "    return [ (token.text, token.lemma_, token.tag_) for token in nlp(sent) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pats(filename):\n",
    "    pat_dict = {}\n",
    "    for line in open(filename, 'r', encoding='utf8'):\n",
    "        try:\n",
    "            nos, pat = line.strip().split('\\t')\n",
    "        except:\n",
    "            # print(\"Exception:\", line)\n",
    "            pass\n",
    "            \n",
    "        for no in nos.split(','):\n",
    "            if no.startswith('#'): continue\n",
    "\n",
    "            no = int(no)\n",
    "            if no in pat_dict:\n",
    "                pass\n",
    "                # print(\"REPEATED:\", no)\n",
    "\n",
    "            pat_dict[no] = pat\n",
    "\n",
    "    return pat_dict\n",
    "\n",
    "\n",
    "re_parentheses = re.compile('\\((?P<info>.*)\\)$')\n",
    "re_level =  re.compile('([ABC][12])')\n",
    "# re_level =  re.compile('(; | |;|^)([ABC][12])( |;|$)') # stricter\n",
    "\n",
    "def read_sents(filename):\n",
    "    sent_dict = {}\n",
    "    \n",
    "    for line in open(filename, 'r', encoding='utf8'):\n",
    "        no, level, sents = line.strip().split('\\t')\n",
    "        no = int(no)\n",
    "        \n",
    "        new_sents = []\n",
    "        for sent in sents.split('|||'):\n",
    "            match = re.search(re_parentheses, sent)\n",
    "            if not match: continue\n",
    "                \n",
    "            info   = match.groupdict()['info']\n",
    "            origin = re.findall(re_level, info)\n",
    "            origin = origin[0] if origin else None\n",
    "            sent   = sent[:match.start()]\n",
    "            \n",
    "            new_sents.append((origin, normalize(sent)))\n",
    "\n",
    "        sent_dict[no] = {'level': level, 'sents': new_sents}\n",
    "\n",
    "    return sent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_patterns(pat_dict, sent_dict):\n",
    "    prev = 0\n",
    "    pat_groups = [[]]\n",
    "    \n",
    "    for no, pat in pat_dict.items():\n",
    "        level = sent_dict[no]['level']\n",
    "        \n",
    "        # create new group\n",
    "        if level_table[level] < prev:\n",
    "            pat_groups.append([])\n",
    "            \n",
    "        # update old group\n",
    "        else:\n",
    "            pat_groups[-1].append((no, level, pat))\n",
    "        \n",
    "        prev = level_table[level]  \n",
    "        \n",
    "    return pat_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used for testing\n",
    "def is_match(sent, pat):\n",
    "    parse = parse_sent(sent)\n",
    "    \n",
    "    words =  ' '.join([x for x, y, z in parse])\n",
    "    lemmas = ' '.join([y.lower() for x, y, z in parse])\n",
    "    tags =   ' '.join([z for x, y, z in parse])\n",
    "\n",
    "    ### rule to catch\n",
    "    stopwords = re.findall('[a-z]+', pat)\n",
    "    lemma_tags = ' '.join([z if y not in stopwords else y for _, y, z in parse])\n",
    "    origin_tags = ' '.join([z if x not in stopwords else x for x, _, z in parse])\n",
    "\n",
    "    return re.search(pat, lemma_tags) or re.search(pat, origin_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the index of start token and end token\n",
    "def align(re_match, tags):\n",
    "    start, end = re_match.span()\n",
    "\n",
    "    length = 0\n",
    "    for i, token in enumerate(tags.split(' ')):\n",
    "        if length >= start: break\n",
    "            \n",
    "        length += len(token) + 1 # space len\n",
    "            \n",
    "    match_len = len(re_match.group().split(' '))\n",
    "    return (i, i+match_len)\n",
    "    \n",
    "\n",
    "def match_pats(sent, pat_groups):\n",
    "    parse = parse_sent(sent)\n",
    "            \n",
    "    words =  ' '.join([x for x, y, z in parse])\n",
    "    lemmas = ' '.join([y.lower() for x, y, z in parse])\n",
    "    tags =   ' '.join([z for x, y, z in parse])\n",
    "\n",
    "    ### rule to catch\n",
    "    group_gets = defaultdict(lambda: [])\n",
    "    for i, group in enumerate(pat_groups):\n",
    "        for each in group:\n",
    "            no, level, pat = each\n",
    "\n",
    "            stopwords = re.findall('[a-z]+', pat)\n",
    "            lemma_tags = ' '.join([z if y not in stopwords else y for _, y, z in parse])\n",
    "            origin_tags = ' '.join([z if x not in stopwords else x for x, _, z in parse])\n",
    "            \n",
    "            lemma_match  = re.search(pat, lemma_tags)\n",
    "            origin_match = re.search(pat, origin_tags)\n",
    "            \n",
    "            if not lemma_match and not origin_match:\n",
    "                continue\n",
    "            elif lemma_match:\n",
    "                start, end = align(lemma_match, lemma_tags)\n",
    "            elif origin_match:\n",
    "                start, end = align(origin_match, origin_tags)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            ngram = ' '.join([el[0] for el in parse[start:end]])\n",
    "            group_gets[i].append((no, level, pat, ngram))\n",
    "            \n",
    "    return group_gets\n",
    "\n",
    "\n",
    "def recommend_pats(group_gets, pat_groups):\n",
    "    group_recs = {}\n",
    "    for i, gets in group_gets.items():\n",
    "        top_level = max(gets, key=lambda x: level_table[x[1]])[1]\n",
    "        recommend = list(filter(lambda el: level_table[top_level] < level_table[el[1]], pat_groups[i]))\n",
    "        group_recs[i] = recommend\n",
    "        \n",
    "    return group_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_table = {\"A1\": 1, \"A2\": 2, \"B1\": 3, \"B2\": 4, \"C1\": 5, \"C2\": 6 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_dict  = read_pats('egp.pattern.txt')\n",
    "sent_dict = read_sents('egp.train.txt')\n",
    "\n",
    "### TEMP\n",
    "delete = [no for no in pat_dict if no > 148 ]\n",
    "for no in delete: del pat_dict[no]\n",
    "delete = [no for no in sent_dict if no not in pat_dict]\n",
    "for no in delete: del sent_dict[no]\n",
    "###\n",
    "\n",
    "pat_groups = group_patterns(pat_dict, sent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(content):\n",
    "    content = normalize(content)\n",
    "    \n",
    "    sent_profiles = []\n",
    "    for sent in nlp(content).sents:\n",
    "        sent = sent.text\n",
    "        \n",
    "        group_gets = match_pats(sent, pat_groups) # match patterns in groups\n",
    "\n",
    "        if not group_gets: continue # non-match\n",
    "        \n",
    "        group_recs  = recommend_pats(group_gets, pat_groups) # recommend patterns in same group\n",
    "        \n",
    "        sent_profiles.append((sent, group_gets, group_recs))\n",
    "    \n",
    "    return sent_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:1315/ (Press CTRL+C to quit)\n",
      "123.195.194.140 - - [19/Feb/2019 00:32:59] \"\u001b[37mOPTIONS /profiling HTTP/1.1\u001b[0m\" 200 -\n",
      "123.195.194.140 - - [19/Feb/2019 00:32:59] \"\u001b[37mPOST /profiling HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I eat an apple. However, it is soar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123.195.194.140 - - [19/Feb/2019 00:36:46] \"\u001b[37mOPTIONS /profiling HTTP/1.1\u001b[0m\" 200 -\n",
      "123.195.194.140 - - [19/Feb/2019 00:36:46] \"\u001b[37mPOST /profiling HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I eat an apple. However, it is soar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123.195.194.140 - - [19/Feb/2019 00:37:02] \"\u001b[37mOPTIONS /profiling HTTP/1.1\u001b[0m\" 200 -\n",
      "123.195.194.140 - - [19/Feb/2019 00:37:02] \"\u001b[37mPOST /profiling HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I eat an apple. However, it is soar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123.195.194.140 - - [19/Feb/2019 00:37:17] \"\u001b[37mOPTIONS /profiling HTTP/1.1\u001b[0m\" 200 -\n",
      "123.195.194.140 - - [19/Feb/2019 00:37:17] \"\u001b[37mPOST /profiling HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I eat an apple. However, it is soar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123.195.194.140 - - [19/Feb/2019 00:38:13] \"\u001b[37mOPTIONS /profiling HTTP/1.1\u001b[0m\" 200 -\n",
      "123.195.194.140 - - [19/Feb/2019 00:38:13] \"\u001b[37mPOST /profiling HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I eat an apple. However, it is soar.\n",
      "I am really a good man.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "CORS(app)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    pass\n",
    "\n",
    "\n",
    "# post /correct data: { content: str }\n",
    "@app.route('/profiling', methods=['POST'])\n",
    "def profiling():\n",
    "    request_data = request.get_json()\n",
    "    if not request_data: return jsonify({'edit': 'Should not be empty'})\n",
    "    \n",
    "    content = request_data['content']\n",
    "    print(content)\n",
    "    \n",
    "    sent_profiles = main(content)\n",
    "\n",
    "    return jsonify({'result': sent_profiles})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=1315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('However, it is soar.',\n",
       "  defaultdict(<function __main__.match_pats.<locals>.<lambda>()>,\n",
       "              {5: [(90, 'A2', '(also|however|so)', 'However'),\n",
       "                (102, 'B1', '(RB|RBR|RBS) ,', 'However ,')],\n",
       "               6: [(116, 'A2', '(so|really|too) VB.*|RB', 'However'),\n",
       "                (119,\n",
       "                 'B1',\n",
       "                 '(VB.* .* RB)|(RB .* VB.*)',\n",
       "                 'However , it is soar .')],\n",
       "               7: [(128, 'A1', 'RB', 'However'),\n",
       "                (133, 'A2', '^RB', 'However'),\n",
       "                (135, 'A2', 'RB', 'However'),\n",
       "                (137, 'A2', '^RB', 'However')],\n",
       "               8: [(142, 'A2', 'RB', 'However')]}),\n",
       "  {5: [(105,\n",
       "     'B2',\n",
       "     '(recently|instantly|shortly|permanently|simultaneously|nowadays)'),\n",
       "    (106, 'B2', '(seriously|urgently|illegally)'),\n",
       "    (107,\n",
       "     'C1',\n",
       "     '(almost|absolutely|awfully|badly|barely|completely|decidedly|deeply|enough|enormously|entirely|extremely|fairly|far|fully|greatly|hardly|highly|how|incredibly|indeed|intensely|just|least|less|little|lots|most|much|nearly|perfectly|positively|practically|pretty|purely|quite|rather|really|scarcely|simply|so|somewhat|strongly|terribly|thoroughly|too|totally|utterly|very|virtually|well) (JJ|JJR)'),\n",
       "    (108,\n",
       "     'C1',\n",
       "     '(absolutely|certainly|conceivably|doubtless|indeed|undeniably|likely|obviously|possibly|probably|really|surely|truly|undoubtedly|unlikely)'),\n",
       "    (109,\n",
       "     'C1',\n",
       "     '(simply|truly|surely|apparently|naturally|surprisingly|inevitably|literally|exceptionally|frankly|clearly|amazingly|wisely|admittedly|apparently|obviously|luckily)')],\n",
       "   6: [(122, 'B2', 'RB (PRP|PRP\\\\$|NN)'),\n",
       "    (123, 'B2', '(almost|very|probably) .* (of|the)'),\n",
       "    (124, 'C1', 'VB.* RB RB'),\n",
       "    (125, 'C1', '(slightly|a bit|much) (JJR|more JJ)')],\n",
       "   7: [(138, 'B2', '^never'), (139, 'C2', '^hardly')],\n",
       "   8: [(143, 'B1', 'RB RB'),\n",
       "    (144, 'B2', '(RBR|more RB)'),\n",
       "    (145, 'C1', '(relatively|extremely) RB'),\n",
       "    (146, 'C1', 'RB enough ,?'),\n",
       "    (147, 'C1', 'RB IN')]})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main('I eat an apple. However, it is soar.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 The latter ĄV fat, ugly and sick ĄV blows his top when Ralph tells the others about the fat boy's nice name, as he wanted to keep it secret.\n",
      "19 For further information, contact Joey Hung.\n",
      "47 I love her because she is friendly.\n",
      "51 Maria realised that being kind and trying to make other people happy is always the best way!\n",
      "94 I'm sorry you can't find it.\n",
      "123 There are probably very few of us who have never been to a zoo.\n",
      "143 [about a film] It came out just yesterday.\n",
      "CPU times: user 11.7 s, sys: 23.7 s, total: 35.3 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for no, entry in sent_dict.items():\n",
    "        level = entry['level']\n",
    "        sents = entry['sents']\n",
    "        \n",
    "        # if no not in patterns_number: continue\n",
    "\n",
    "        for origin_level, sent in sents:\n",
    "            if is_match(sent, pat_dict[no]):\n",
    "                pass\n",
    "            else:\n",
    "                print(no, sent)\n",
    "                \n",
    "            # main process\n",
    "#             print(sent)\n",
    "#             group_gets = match_pats(sent, pat_groups) # match patterns in groups\n",
    "#             print(group_gets)\n",
    "#             group_recs  = recommend_pats(group_gets, pat_groups) # recommend patterns in same group\n",
    "#             print(group_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "79  3.93 s\n",
    "148 7.63 s\n",
    "prog = re.compile(pattern)\n",
    "result = prog.match(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sent(\"The cars will fly around the buildings like planes, and they will be faster and cheaper.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
