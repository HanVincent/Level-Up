{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.preprocess import normalize\n",
    "from utils.parser import nlp\n",
    "from utils.extract import clean_content\n",
    "from utils.auto_suggest import auto_suggest, suggest_sentences\n",
    "from utils.grammar import generate_candidates, iterate_all_patterns, iterate_all_gets, remove_overlap\n",
    "from utils.vocabulary import level_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_suggesting(content):\n",
    "    content = content.strip()\n",
    "    if not content: return None, [] # empty content\n",
    "\n",
    "    # get sentences\n",
    "    sentences = list(nlp(content, disable=['ner']).sents)\n",
    "    last_sent = sentences[-1]\n",
    "    \n",
    "    # normalize\n",
    "    content = normalize(last_sent.text)\n",
    "    parse = nlp(content, disable=['ner'])\n",
    "\n",
    "    # 1. generate possible sentences\n",
    "    parses = generate_candidates(parse)\n",
    "    \n",
    "    # 2. find patterns for each candidate\n",
    "    gets = [get for parse in parses for get in iterate_all_patterns(parse)]\n",
    "\n",
    "    # 3. remove duplicate\n",
    "    gets = remove_overlap(parse, gets)\n",
    "\n",
    "    # 4. recommend related higher pattern in the same group\n",
    "    get, sugs = auto_suggest(parse, gets)\n",
    "    \n",
    "    return get, sugs\n",
    "\n",
    "    \n",
    "def main_profiling(content):\n",
    "    sentence_profiles = []\n",
    "    for sent in nlp(content, disable=['ner']).sents:\n",
    "        # 0. parse sentence\n",
    "        parse = nlp(normalize(sent.text), disable=['ner'])\n",
    "        \n",
    "        # 1. generate possible sentences\n",
    "        parses = generate_candidates(parse)\n",
    "        \n",
    "        # 2. find patterns for each candidate\n",
    "        gets = [get for parse in parses for get in iterate_all_patterns(parse)]\n",
    "\n",
    "        # 3. remove duplicate\n",
    "        gets = remove_overlap(parse, gets)\n",
    "        \n",
    "        # 4. recommend related higher pattern in the same group\n",
    "        recs = iterate_all_gets(parse, gets)\n",
    "        \n",
    "        # 5. return\n",
    "        sentence_profiles.append({'sent': sent.text, \n",
    "                                  'parse': [tk.text for tk in parse],\n",
    "                                  'gets': gets, 'recs': recs })\n",
    "\n",
    "    return sentence_profiles\n",
    "\n",
    "\n",
    "def main_vocabuing(sentence):\n",
    "    parse = nlp(normalize(sentence))\n",
    "    vocabs = level_vocab(parse)\n",
    "    \n",
    "    return vocabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vocabulary import recommend_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'category': 'ADJECTIVES',\n",
      "  'example': 'he looked as if he belonged in the chic , <w>attractive</w> '\n",
      "             '<w>,</w> <w>fashionable</w> <w>neighborhood</w> around q street '\n",
      "             '.',\n",
      "  'level': 'B1',\n",
      "  'no': 3,\n",
      "  'statement': 'Can use a comma to combine two adjectives used before the '\n",
      "               'noun, following the usual order of adjective types.',\n",
      "  'subcategory': 'combining'},\n",
      " None,\n",
      " {'category': 'ADJECTIVES',\n",
      "  'example': \"anyway i did n't feel triumphant in any way i -- i did n't think \"\n",
      "             '-- but i did <w>feel</w> <w>ugly</w> .',\n",
      "  'level': 'A2',\n",
      "  'no': 48,\n",
      "  'statement': 'Can use a limited range of adjectives predicatively, after '\n",
      "               \"linking verbs 'look' and 'feel'.\",\n",
      "  'subcategory': 'position'},\n",
      " {'category': 'ADJECTIVES',\n",
      "  'example': 'by the end of the day we should have a <w>complete</w> '\n",
      "             '<w>house</w> -- the silhouette of the house totally in place .',\n",
      "  'level': 'B2',\n",
      "  'no': 53,\n",
      "  'statement': \"Can use a limited range of degree adjectives ('real', \"\n",
      "               \"'absolute', 'complete') before a noun to express intensity.\",\n",
      "  'subcategory': 'position'},\n",
      " {'category': 'ADVERBS',\n",
      "  'example': 'the eyes i had managed to <w>wrong</w> <w>so</w> deeply .',\n",
      "  'level': 'A2',\n",
      "  'no': 116,\n",
      "  'statement': 'Can use a limited range of manner adverbs to modify verbs.',\n",
      "  'subcategory': 'adverbs and adverb phrases: types and meanings'},\n",
      " None,\n",
      " None,\n",
      " None,\n",
      " {'category': 'ADVERBS',\n",
      "  'example': 'though it used fuel efficiently , <w>it</w> <w>did</w> <w>so</w> '\n",
      "             '<w>p155</w> at a high cost in electromagnetic energy .',\n",
      "  'level': 'A2',\n",
      "  'no': 134,\n",
      "  'statement': 'Can use adverbs in mid position between the subject and the '\n",
      "               \"main verb and after modal verbs, auxiliary verbs and 'be'.\",\n",
      "  'subcategory': 'position'},\n",
      " None]\n"
     ]
    }
   ],
   "source": [
    "parse = nlp(\"Amazingly , the child is so fashionable and creative that he makes the ugly house modern.\")\n",
    "group_gets = iterate_all_patterns(parse)\n",
    "from pprint import pprint\n",
    "# pprint(group_gets)\n",
    "# from pprint import pprint\n",
    "# pprint(group_gets)\n",
    "# gets = remove_overlap(parse, group_gets)\n",
    "\n",
    "group_recs  = iterate_all_gets(parse, group_gets) # recommend patterns in same group\n",
    "pprint(group_recs)\n",
    "# print(main_suggesting(\"I go the swimming pool two time a week, so I am goodthissport.Could you tell me what sort of clothes I will have to wear to play tennis? Amazingly, the child is so fashionable and creative that he makes the ugly house modern.  \"))\n",
    "\n",
    "# for tk in parse:\n",
    "#     print(tk, tk.lemma_, tk.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "CORS(app)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "# post /suggesting data: { content: str }\n",
    "@app.route('/suggesting', methods=['POST'])\n",
    "def suggesting():\n",
    "    request_data = request.get_json()\n",
    "    if not request_data: return jsonify({'result': 'Should not be empty'})\n",
    "\n",
    "    content = request_data['content']\n",
    "    get, suggestions = main_suggesting(content)\n",
    "\n",
    "    return jsonify({'get': get, 'suggestions': suggestions})\n",
    "\n",
    "\n",
    "@app.route('/examples', methods=['POST'])\n",
    "def examples():\n",
    "    request_data = request.get_json()\n",
    "    if not request_data: return jsonify({'result': 'Should not be empty'})\n",
    "\n",
    "    ngram = request_data['ngram']\n",
    "    sentences = suggest_sentences(ngram)\n",
    "\n",
    "    return jsonify({'ngram': ngram, 'examples': sentences})\n",
    "\n",
    "\n",
    "# post /profiling data: { content: str , access: str}\n",
    "@app.route('/profiling', methods=['POST'])\n",
    "def profiling():\n",
    "    request_data = request.get_json()\n",
    "    if not request_data: return jsonify({'result': 'Should not be empty'})\n",
    "\n",
    "    if request_data['access'] == 'url':\n",
    "        content = clean_content(request_data['content']) # url\n",
    "    else:\n",
    "        content = request_data['content']\n",
    "        \n",
    "    # print(content)\n",
    "    sent_profiles = main_profiling(content)\n",
    "\n",
    "    return jsonify({'profiles': sent_profiles})\n",
    "\n",
    "\n",
    "# post /vocabuing data: { sentence: str }\n",
    "@app.route('/vocabuing', methods=['POST'])\n",
    "def vocabuing():\n",
    "    request_data = request.get_json()\n",
    "    if not request_data: return jsonify({'result': 'Should not be empty'})\n",
    "    \n",
    "    sentence = request_data['sentence']\n",
    "    \n",
    "    vocabs = main_vocabuing(sentence)\n",
    "\n",
    "    return jsonify({'vocabs': vocabs})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=1316)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egs = []\n",
    "\n",
    "# for index, entry in Egp.get_examples().items():\n",
    "#     if index not in Egp.get_patterns(): continue\n",
    "        \n",
    "#     eg = []\n",
    "#     for sent in entry['sents']:\n",
    "#         level, sent = sent\n",
    "#         parse = nlp(normalize(sent))\n",
    "    \n",
    "#         matches = match_pat(parse, index, Egp.get_patterns()[index])\n",
    "#         if not matches: continue\n",
    "            \n",
    "#         sent = []\n",
    "#         for tk in parse:\n",
    "#             starts = [match[0] for match in matches]\n",
    "#             ends = [match[1] for match in matches]                \n",
    "\n",
    "#             if tk.i in starts:\n",
    "#                 sent.extend(['<w>', tk.text])\n",
    "#             elif tk.i in ends:\n",
    "#                 sent.extend(['</w>', tk.text])\n",
    "#             else:\n",
    "#                 sent.append(tk.text)\n",
    "        \n",
    "#         sent = ['I' if tk == 'i' else tk for tk in sent]\n",
    "#         sent = ' '.join(sent)\n",
    "#         eg.append(sent)\n",
    "\n",
    "#     if not eg: egs.append((index, entry['sents'][0][1]))\n",
    "#     else:      egs.append((index, eg[0]))\n",
    "        \n",
    "# with open('egp.highlights.txt', 'w', encoding='utf8') as ws:\n",
    "#     for line in egs:\n",
    "#         print(*line, sep='\\t', file=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e3894dccef71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlevel_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-ad0ad4f2abf3>\u001b[0m in \u001b[0;36mlevel_vocab\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlevel_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mannotate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'level'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEvp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecommend_vocabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mannotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ad0ad4f2abf3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlevel_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mannotate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'level'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEvp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecommend_vocabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mannotate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
