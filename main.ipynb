{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import sys, os, re\n",
    "from math import log\n",
    "from itertools import product\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords # might use spacy stopword\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sent(sent):\n",
    "    # strip and replace multiple spaces\n",
    "    return ' '.join(sent.split())\n",
    "\n",
    "\n",
    "def parse_sent(text):\n",
    "    doc = nlp(text.strip())\n",
    "    \n",
    "    # TODO: why nlp sent.text again?\n",
    "    return [ (token.text, token.lemma_, token.tag_) for sent in doc.sents for token in nlp(sent.text) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pats(filename):\n",
    "    pat_dict = {}\n",
    "    for line in open(filename, 'r', encoding='utf8'):\n",
    "        try:\n",
    "            nos, pat = line.strip().split('\\t')\n",
    "        except:\n",
    "            # print(\"Exception:\", line)\n",
    "            pass\n",
    "            \n",
    "        for no in nos.split(','):\n",
    "            if no.startswith('#'): continue\n",
    "\n",
    "            no = int(no)\n",
    "            if no in pat_dict:\n",
    "                pass\n",
    "                # print(\"REPEATED:\", no)\n",
    "\n",
    "            pat_dict[no] = re.compile(pat)\n",
    "\n",
    "    return pat_dict\n",
    "\n",
    "\n",
    "re_parentheses = re.compile('\\((?P<info>.*)\\)$')\n",
    "re_level =  re.compile('([ABC][12])')\n",
    "# re_level =  re.compile('(; | |;|^)([ABC][12])( |;|$)') # stricter\n",
    "\n",
    "def read_sents(filename):\n",
    "    sent_dict = {}\n",
    "    \n",
    "    for line in open(filename, 'r', encoding='utf8'):\n",
    "        no, level, sents = line.strip().split('\\t')\n",
    "        no = int(no)\n",
    "        \n",
    "        new_sents = []\n",
    "        for sent in sents.split('|||'):\n",
    "            match = re.search(re_parentheses, sent)\n",
    "            if not match: continue\n",
    "                \n",
    "            info   = match.groupdict()['info']\n",
    "            origin = re.findall(re_level, info)\n",
    "            origin = origin[0] if origin else None\n",
    "            sent   = sent[:match.start()]\n",
    "            \n",
    "            new_sents.append((origin, norm_sent(sent)))\n",
    "\n",
    "        sent_dict[no] = {'level': level, 'sents': new_sents}\n",
    "\n",
    "    return sent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_patterns(pat_dict, sent_dict):\n",
    "    prev = 0\n",
    "    pat_groups = [[]]\n",
    "    \n",
    "    for no, pat in pat_dict.items():\n",
    "        level = sent_dict[no]['level']\n",
    "        \n",
    "        # create new group\n",
    "        if level_table[level] < prev:\n",
    "            pat_groups.append([])\n",
    "            \n",
    "        # update old group\n",
    "        else:\n",
    "            pat_groups[-1].append((no, level, pat))\n",
    "        \n",
    "        prev = level_table[level]  \n",
    "        \n",
    "    return pat_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used for testing\n",
    "def is_match(sent, pat):\n",
    "    parse = parse_sent(sent)\n",
    "    \n",
    "    words =  ' '.join([x for x, y, z in parse])\n",
    "    lemmas = ' '.join([y.lower() for x, y, z in parse])\n",
    "    tags =   ' '.join([z for x, y, z in parse])\n",
    "\n",
    "    ### rule to catch\n",
    "    stopwords = re.findall('[a-z]+', pat)\n",
    "    lemma_tags = ' '.join([z if y not in stopwords else y for _, y, z in parse])\n",
    "    origin_tags = ' '.join([z if x not in stopwords else x for x, _, z in parse])\n",
    "\n",
    "    return re.search(pat, lemma_tags) or re.search(pat, origin_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(sent, pat_groups):\n",
    "    parse = parse_sent(sent)\n",
    "            \n",
    "    words =  ' '.join([x for x, y, z in parse])\n",
    "    lemmas = ' '.join([y.lower() for x, y, z in parse])\n",
    "    tags =   ' '.join([z for x, y, z in parse])\n",
    "\n",
    "    ### rule to catch\n",
    "    all_gets = set()\n",
    "    for group in pat_groups:\n",
    "        group_gets = []\n",
    "        for each in group:\n",
    "            no, level, pat = each\n",
    "\n",
    "            stopwords = re.findall('[a-z]+', pat)\n",
    "            lemma_tags = ' '.join([z if y not in stopwords else y for _, y, z in parse])\n",
    "            origin_tags = ' '.join([z if x not in stopwords else x for x, _, z in parse])\n",
    "\n",
    "            if re.search(pat, lemma_tags) or re.search(pat, origin_tags): \n",
    "                group_gets.append((level, pat)) # no\n",
    "                all_gets.add(level)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if group_gets:\n",
    "#             print(\"Got:\\t\", group_gets)\n",
    "            top_level = sorted(group_gets, key=lambda x: x[0], reverse=True)[0][0]\n",
    "\n",
    "            recommend = list(filter(lambda el: level_table[top_level] < level_table[el[1]], group))\n",
    "#             print(\"Rec:\\t\", recommend)\n",
    "            \n",
    "#     print(sorted(all_gets, key=lambda x: level_table[level]))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_table = {\"A1\": 1, \"A2\": 2, \n",
    "               \"B1\": 3, \"B2\": 4, \n",
    "               \"C1\": 5, \"C2\": 6 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_dict  = read_pats('egp.pattern.txt')\n",
    "sent_dict = read_sents('egp.train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 The latter ĄV fat, ugly and sick ĄV blows his top when Ralph tells the others about the fat boy's nice name, as he wanted to keep it secret.\n",
      "19 For further information, contact Joey Hung.\n",
      "47 I love her because she is friendly.\n",
      "51 Maria realised that being kind and trying to make other people happy is always the best way!\n",
      "94 I'm sorry you can't find it.\n",
      "143 [about a film] It came out just yesterday.\n",
      "CPU times: user 50 s, sys: 1min 42s, total: 2min 32s\n",
      "Wall time: 7.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ### TEMP\n",
    "    delete = [no for no in pat_dict if no > 148 ]\n",
    "    for no in delete: del pat_dict[no]\n",
    "    delete = [no for no in sent_dict if no not in pat_dict]\n",
    "    for no in delete: del sent_dict[no]\n",
    "    ###\n",
    "    \n",
    "    pat_groups = group_patterns(pat_dict, sent_dict)\n",
    "    \n",
    "    for no, entry in sent_dict.items():\n",
    "        level = entry['level']\n",
    "        sents = entry['sents']\n",
    "        \n",
    "        # if no not in patterns_number: continue\n",
    "\n",
    "        for origin_level, sent in sents:\n",
    "            if is_match(sent, pat_dict[no]):\n",
    "                pass\n",
    "            else:\n",
    "                print(no, sent)\n",
    "            recommend(sent, pat_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "79  3.93 s\n",
    "148 7.63 s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_sent(\"It came out just yesterday.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
