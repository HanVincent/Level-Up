{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from math import log\n",
    "from itertools import groupby\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    import re\n",
    "    \n",
    "    infix_re  = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import *\n",
    "from utils.grammar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.EGP import EGP\n",
    "\n",
    "Egp = EGP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Dictionary import Dictionary\n",
    "Dict = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used for testing\n",
    "import re\n",
    "re_token = re.compile('\\w+|[,.:;!?]')\n",
    "def is_match(parse, pat):\n",
    "    ### rule to catch\n",
    "    stopwords = re_token.findall(pat.pattern)\n",
    "    norm_tags  = ' '.join([tk.tag_ if tk.norm_ not in stopwords else tk.norm_ for tk in parse])\n",
    "    lemma_tags  = ' '.join([get_lemma(tk, stopwords) for tk in parse])\n",
    "    origin_tags = ' '.join([tk.tag_ if tk.text not in stopwords else tk.text for tk in parse])\n",
    "\n",
    "    return pat.search(norm_tags) or pat.search(lemma_tags) or pat.search(origin_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_vocab(parse):\n",
    "    annotate = [(tk.text, Dict.lookup(tk.lemma_)) for tk in parse]\n",
    "    return annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_profiling(content):\n",
    "    # content = normalize(content)\n",
    "    \n",
    "    sent_profiles = []\n",
    "    for sent in nlp(content).sents:\n",
    "        parse = nlp(normalize(sent.text))\n",
    "        \n",
    "        # 1. find non-overlapped matches\n",
    "        gets = iterate_pats(parse, pat_groups) # match patterns in groups\n",
    "        # print(gets)\n",
    "        # if not gets: continue # non-match\n",
    "        \n",
    "        # 2. recommend related higher pattern in the same group\n",
    "        recs  = recommend_pats(gets, pat_groups)\n",
    "        # print(recs)\n",
    "        \n",
    "        sent_profiles.append({'sent': sent.text, 'parse': ' '.join([tk.text for tk in parse]), \n",
    "                              'gets': gets, 'recs': recs })\n",
    "\n",
    "    return sent_profiles\n",
    "\n",
    "\n",
    "def main_vocabuing(sentence):\n",
    "    sentence = normalize(sentence)\n",
    "    parse = nlp(sentence)\n",
    "\n",
    "    # 1. get vocabulary level\n",
    "    vocabs = level_vocab(parse)\n",
    "    \n",
    "    return vocabs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_dict  = Egp.get_patterns()\n",
    "sent_dict = Egp.get_examples()\n",
    "\n",
    "### TEMP\n",
    "# delete = [no for no in pat_dict] # no < 1020 or no > 1050\n",
    "# for no in delete: del pat_dict[no]\n",
    "delete = [no for no in sent_dict if no not in pat_dict]\n",
    "for no in delete: del sent_dict[no]\n",
    "###\n",
    "\n",
    "pat_groups = Egp.get_group_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_gets = iterate_pats(nlp(\"long\"), pat_groups)\n",
    "\n",
    "# # group_recs  = recommend_pats(group_gets, pat_groups) # recommend patterns in same group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     for no, entry in sent_dict.items():\n",
    "#         level = entry['level']\n",
    "#         sents = entry['sents']\n",
    "        \n",
    "#         # if no not in patterns_number: continue\n",
    "\n",
    "#         for origin_level, sent in sents:\n",
    "# #             parse = nlp(sent)\n",
    "# #             if is_match(parse, pat_dict[no]):\n",
    "# #                 pass\n",
    "# #             else:\n",
    "# #                 print(no, pat_dict[no].pattern, sent)\n",
    "                \n",
    "#             # main process\n",
    "#             print(sent)\n",
    "#             group_gets = iterate_pats(parse, pat_groups) # match patterns in groups\n",
    "#             print(group_gets)\n",
    "#             group_recs  = recommend_pats(group_gets, pat_groups) # recommend patterns in same group\n",
    "#             print(group_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(\"it is the biggest and oldest museum in libya . … it is the biggest and <w> oldest museum </w> in libya .\")\n",
    "# for a in doc:\n",
    "#     print(a.text, a.lemma_, a.norm_, a.tag_, a.pos_, a.i)\n",
    "# doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from flask_cors import CORS, cross_origin\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.config['CORS_HEADERS'] = 'Content-Type'\n",
    "CORS(app)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    pass\n",
    "\n",
    "\n",
    "# post /profiling data: { content: str }\n",
    "@app.route('/profiling', methods=['POST'])\n",
    "def profiling():\n",
    "    request_data = request.get_json()\n",
    "    if not request_data: return jsonify({'result': 'Should not be empty'})\n",
    "    \n",
    "    content = request_data['content']\n",
    "    print(content)\n",
    "    \n",
    "    sent_profiles = main_profiling(content)\n",
    "\n",
    "    return jsonify({'profiles': sent_profiles})\n",
    "\n",
    "\n",
    "# post /vocabuing data: { sentence: str }\n",
    "@app.route('/vocabuing', methods=['POST'])\n",
    "def vocabuing():\n",
    "    request_data = request.get_json()\n",
    "    if not request_data: return jsonify({'result': 'Should not be empty'})\n",
    "    \n",
    "    sentence = request_data['sentence']\n",
    "    \n",
    "    vocabs = main_vocabuing(sentence)\n",
    "\n",
    "    return jsonify({'vocabs': vocabs})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=1315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egs = []\n",
    "\n",
    "# for index, entry in Egp.get_examples().items():\n",
    "#     if index not in pat_dict: continue\n",
    "        \n",
    "#     eg = []\n",
    "#     for sent in entry['sents']:\n",
    "#         level, sent = sent\n",
    "#         parse = nlp(normalize(sent))\n",
    "    \n",
    "#         matches = match_pat(parse, index, pat_dict[index])\n",
    "#         if not matches: continue\n",
    "            \n",
    "#         sent = []\n",
    "#         for tk in parse:\n",
    "#             starts = [match[0] for match in matches]\n",
    "#             ends = [match[1] for match in matches]                \n",
    "\n",
    "#             if tk.i in starts:\n",
    "#                 sent.extend(['<w>', tk.text])\n",
    "#             elif tk.i in ends:\n",
    "#                 sent.extend(['</w>', tk.text])\n",
    "#             else:\n",
    "#                 sent.append(tk.text)\n",
    "        \n",
    "#         sent = ['I' if tk == 'i' else tk for tk in sent]\n",
    "#         sent = ' '.join(sent)\n",
    "#         eg.append(sent)\n",
    "\n",
    "#     if not eg: egs.append((index, entry['sents'][0][1]))\n",
    "#     else:      egs.append((index, eg[0]))\n",
    "        \n",
    "# with open('egp.highlights.txt', 'w', encoding='utf8') as ws:\n",
    "#     for line in egs:\n",
    "#         print(*line, sep='\\t', file=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
