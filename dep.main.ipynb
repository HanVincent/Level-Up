{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os, re\n",
    "from math import log\n",
    "from itertools import product\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords # might use spacy stopword\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re  = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import *\n",
    "from utils.io import *\n",
    "from utils.preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_patterns(pat_dict, sent_dict):\n",
    "    prev = 0\n",
    "    pat_groups = [[]]\n",
    "    \n",
    "    for no, pat in pat_dict.items():\n",
    "        level = sent_dict[no]['level']\n",
    "        \n",
    "        # create new group\n",
    "        if level_table[level] < prev:\n",
    "            pat_groups.append([])\n",
    "            \n",
    "        # update old group\n",
    "        else:\n",
    "            pat_groups[-1].append((no, level, pat))\n",
    "        \n",
    "        prev = level_table[level]  \n",
    "        \n",
    "    return pat_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only used for testing\n",
    "re_token = re.compile('([a-z-]+)|[,\\.:;!\\?]')\n",
    "def is_match(sent, pat):\n",
    "    parse = nlp(sent)\n",
    "    \n",
    "    ### rule to catch\n",
    "    stopwords = re_token.findall(pat.pattern)\n",
    "    lemma_tags  = ' '.join([tk.tag_ if tk.lemma_ not in stopwords else tk.lemma_ for tk in parse])\n",
    "    origin_tags = ' '.join([tk.tag_ if tk.text not in stopwords else tk.text for tk in parse])\n",
    "\n",
    "    return pat.search(lemma_tags) or pat.search(origin_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the index of start token and end token\n",
    "def align(re_match, tags):\n",
    "    start, end = re_match.span()\n",
    "\n",
    "    length = 0\n",
    "    for i, token in enumerate(tags.split(' ')):\n",
    "        if length >= start: break\n",
    "            \n",
    "        length += len(token) + 1 # space len\n",
    "            \n",
    "    match_len = len(re_match.group().split(' '))\n",
    "    return (i, i+match_len)\n",
    "    \n",
    "\n",
    "re_token = re.compile('[a-z-]+|[,\\.:;!\\?]')\n",
    "def match_pat(parse, pat):\n",
    "    \n",
    "    stopwords = re_token.findall(pat.pattern)\n",
    "    lemma_tags  = ' '.join([tk.tag_ if tk.lemma_ not in stopwords else tk.lemma_ for tk in parse])\n",
    "    origin_tags = ' '.join([tk.tag_ if tk.text not in stopwords else tk.text for tk in parse])\n",
    "\n",
    "    \n",
    "    lemma_match  = pat.search(lemma_tags)\n",
    "    origin_match = pat.search(origin_tags)\n",
    "\n",
    "    if lemma_match:\n",
    "        start, end = align(lemma_match, lemma_tags)\n",
    "        return True, (start, end)\n",
    "    elif origin_match:\n",
    "        start, end = align(origin_match, origin_tags)\n",
    "        return True, (start, end)\n",
    "    \n",
    "    return False, (0, 0)\n",
    "\n",
    "\n",
    "def iterate_pats(sent, pat_groups):\n",
    "    parse = nlp(sent)\n",
    "            \n",
    "    ### rule to catch\n",
    "    group_gets = {}\n",
    "    for i, group in enumerate(pat_groups):\n",
    "        gets = []\n",
    "        for each in group:\n",
    "            no, level, pat = each\n",
    "\n",
    "            is_match, (start, end) = match_pat(parse, pat)\n",
    "            if not is_match: continue\n",
    "            \n",
    "            ngram = ' '.join([el.text for el in parse[start:end]])\n",
    "            gets.append((no, level, pat.pattern, ngram))\n",
    "        \n",
    "        if not gets: continue\n",
    "        \n",
    "        print(gets)\n",
    "        \n",
    "        gets.sort(key=lambda el: len(el[3].split())) # sort by length\n",
    "        get = max(gets, key=lambda el: el[1]) # max level\n",
    "        group_gets[i] = get\n",
    "        \n",
    "    return group_gets\n",
    "\n",
    "\n",
    "def recommend_pats(group_gets, pat_groups):\n",
    "    group_recs = {}\n",
    "    for i, get in group_gets.items():\n",
    "        no, level, pat, ngram = get\n",
    "        \n",
    "        recs = filter(lambda el: level_table[level] < level_table[el[1]], pat_groups[i])\n",
    "        recs = map(lambda el: (el[0], el[1], el[2].pattern), recs)\n",
    "        group_recs[i] = list(recs)\n",
    "        \n",
    "    return group_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_dict  = read_pats('egp.regex.pattern.txt')\n",
    "sent_dict = read_sents('egp.train.txt')\n",
    "\n",
    "### TEMP\n",
    "delete = [no for no in pat_dict if no > 149 ]\n",
    "for no in delete: del pat_dict[no]\n",
    "delete = [no for no in sent_dict if no not in pat_dict]\n",
    "for no in delete: del sent_dict[no]\n",
    "###\n",
    "\n",
    "pat_groups = group_patterns(pat_dict, sent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(content):\n",
    "    content = normalize(content)\n",
    "    \n",
    "    sent_profiles = []\n",
    "    for sent in nlp(content).sents:\n",
    "        sent = sent.text\n",
    "        \n",
    "        group_gets = iterate_pats(sent, pat_groups) # match patterns in groups\n",
    "\n",
    "        if not group_gets: continue # non-match\n",
    "        \n",
    "        group_recs  = recommend_pats(group_gets, pat_groups) # recommend patterns in same group\n",
    "        \n",
    "        sent_profiles.append((sent, group_gets, group_recs))\n",
    "    \n",
    "    return sent_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate_pats(\"There have been so many embarrassing moments in my life. It's very difficult to pick the most embarrassing.\", pat_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for no, entry in sent_dict.items():\n",
    "        level = entry['level']\n",
    "        sents = entry['sents']\n",
    "        \n",
    "        # if no not in patterns_number: continue\n",
    "\n",
    "        for origin_level, sent in sents:\n",
    "            if is_match(sent, pat_dict[no]):\n",
    "                pass\n",
    "            else:\n",
    "                print(no, pat_dict[no].pattern, sent)\n",
    "                \n",
    "            # main process\n",
    "#             print(sent)\n",
    "#             group_gets = iterate_pats(sent, pat_groups) # match patterns in groups\n",
    "#             print(group_gets)\n",
    "#             group_recs  = recommend_pats(group_gets, pat_groups) # recommend patterns in same group\n",
    "#             print(group_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_pat(parse_sent(\"Who cares?\"), re.compile('^who VB.? \\?$'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep tree       Token     Dep type Lemma     Norm      Part of Sp tag\n",
      "────────────── ───────── ──────── ───────── ───────── ────────── ───\n",
      "           ┌─► The       det      the       The       DET        DT \n",
      "        ┌─►└── house     nsubj    house     house     NOUN       NN \n",
      "┌┬──────┴┬┬┬── is        ROOT     be        is        VERB       VBZ\n",
      "││       ││└─► beautiful acomp    beautiful beautiful ADJ        JJ \n",
      "││       │└──► ,         punct    ,         ,         PUNCT      ,  \n",
      "││       └───► and       cc       and       and       CCONJ      CC \n",
      "││         ┌─► so        advmod   so        so        ADV        RB \n",
      "││      ┌─►└── much      advmod   much      much      ADV        RB \n",
      "│└─►┌───┴───── bigger    conj     big       bigger    ADJ        JJR\n",
      "│   └─►┌────── than      prep     than      than      ADP        IN \n",
      "│      │  ┌──► the       det      the       the       DET        DT \n",
      "│      │  │┌─► previous  amod     previous  previous  ADJ        JJ \n",
      "│      └─►└┴── one       pobj     one       one       NUM        CD \n",
      "└────────────► .         punct    .         .         PUNCT      .  \n"
     ]
    }
   ],
   "source": [
    "from utils.explacy import *\n",
    "\n",
    "explacy.print_parse_info(nlp, \"The house is beautiful, and so much bigger than the previous one.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It -PRON- it [was] 0\n",
      "was be was [] 1\n",
      "the the the [TV, was] 0\n",
      "biggest big biggest [TV, was] 0\n",
      "TV tv TV [was] 2\n",
      "in in in [TV, was] 0\n",
      "the the the [bazaar, in, TV, was] 0\n",
      "bazaar bazaar bazaar [in, TV, was] 1\n",
      ", , , [TV, was] 0\n",
      "with with with [TV, was] 0\n",
      "its -PRON- its [screen, with, TV, was] 0\n",
      "huge huge huge [screen, with, TV, was] 0\n",
      ", , , [screen, with, TV, was] 0\n",
      "black black black [screen, with, TV, was] 0\n",
      "screen screen screen [with, TV, was] 4\n",
      ". . . [was] 0\n"
     ]
    }
   ],
   "source": [
    "for a in nlp(\"It was the biggest TV in the bazaar, with its huge, black screen.\"):\n",
    "    print(a.text, a.lemma_, a.norm_, list(a.ancestors), a.n_lefts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
