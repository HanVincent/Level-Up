{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    import re\n",
    "    \n",
    "    infix_re  = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg') \n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from utils.preprocess import normalize\n",
    "from utils.grammar import generate_candidates, iterate_all_patterns, iterate_all_gets\n",
    "from utils.vocabulary import level_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotDict(dict):\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "    \n",
    "    \n",
    "def construct(block):\n",
    "    lines = [line for line in block.split('\\n') if line]\n",
    "    if len(lines) < 2: return []\n",
    "\n",
    "    text, lines = lines[0].replace('# text = ', ''), lines[1:]\n",
    "    parses = [ DotDict() for _ in range(len(lines)) ]\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        index, token, lemma, norm, pos, tag, dep, head, children = line.split('\\t')\n",
    "        children = children.strip()\n",
    "        \n",
    "        parses[i].update({\n",
    "            'i': int(index),\n",
    "            'text': token,\n",
    "            'norm_': norm,\n",
    "            'lemma_': lemma,\n",
    "            'dep_': dep,\n",
    "            'pos_': pos,\n",
    "            'tag_': tag,\n",
    "            'head': parses[int(head)], # if head = -1, it's root\n",
    "            'children': [parses[int(e)] for e in children.split(',')] if children else [],\n",
    "            'doc': parses\n",
    "        })\n",
    "        \n",
    "    return parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gzip.open('bnc.parse.txt.gz', 'rt', encoding='utf8')\n",
    "# fs = open('bnc.parse.txt', 'r', encoding='utf8')\n",
    "contents = fs.read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = defaultdict(Counter)\n",
    "sentences = defaultdict(lambda: defaultdict(lambda: []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 5190445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-651089415e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 2. find patterns for each candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparses\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_all_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 3. remove duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-651089415e17>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 2. find patterns for each candidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparses\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_all_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 3. remove duplicate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cam.evp.egp/utils/grammar.py\u001b[0m in \u001b[0;36miterate_all_patterns\u001b[0;34m(parse, pat_groups)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# match pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cam.evp.egp/utils/grammar.py\u001b[0m in \u001b[0;36mmatch_pattern\u001b[0;34m(parse, no)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mlemma_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_lemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0morigin_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cam.evp.egp/utils/grammar.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mlemma_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_lemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0morigin_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total = len(contents)\n",
    "\n",
    "for i, entry in enumerate(contents):\n",
    "    parse = construct(entry)\n",
    "\n",
    "    # 1. generate possible sentences\n",
    "    parses = generate_candidates(parse)\n",
    "\n",
    "    # 2. find patterns for each candidate\n",
    "    gets = [get for parse in parses for get in iterate_all_patterns(parse)]\n",
    "\n",
    "    # 3. remove duplicate\n",
    "    uniq_gets = []\n",
    "    [uniq_gets.append(get) for get in gets if get not in uniq_gets]\n",
    "    gets = uniq_gets\n",
    "    \n",
    "    for get in uniq_gets:\n",
    "        # counters[get['no']][get['match']][get['ngram']] += 1\n",
    "        counters[get['no']][ get['match'] + '|' + get['ngram'] ] += 1\n",
    "        sentences[get['no']][ get['match'] + '|' + get['ngram'] ].append(\n",
    "            ' '.join([ '<w>' + tk.text + '</w>' if tk.i in get['indices'] else tk.text for tk in parse ])\n",
    "        )\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print('{} / {}'.format(i, total))\n",
    "        with open('counters.json', 'w') as f:\n",
    "            json.dump(counters, f)\n",
    "\n",
    "        with open('sentences.json', 'w') as f:\n",
    "            json.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('counters.json', 'r') as f:\n",
    "#     counters = json.load(f)\n",
    "\n",
    "# with open('sentences.json', 'r') as f:\n",
    "#     sentences = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
