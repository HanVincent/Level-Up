{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('../uncased_L-12_H-768_A-12')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('../uncased_L-12_H-768_A-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    text = \"[CLS] \" + sent + \" [SEP]\"\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "    segments_ids = [0]*len(indexed_tokens)\n",
    "\n",
    "    return tokenized_text, indexed_tokens, segments_ids\n",
    "\n",
    "\n",
    "def get_representation(indexed_tokens, segments_ids):\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # If you have a GPU, put everything on cuda\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    segments_tensors = segments_tensors.to('cuda')\n",
    "    model.to('cuda')\n",
    "    \n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "        return encoded_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = json.load(open('data/dict.json', 'r', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_pairs, examples = [], []\n",
    "\n",
    "for word in dictionary:\n",
    "    for pos in dictionary[word]:\n",
    "        for each in dictionary[word][pos]: # list\n",
    "            info_pairs.append((word, each['definition']))\n",
    "            try:    \n",
    "                examples.append(each['dic_examples'][0])\n",
    "            except:\n",
    "                examples.append(\"\")\n",
    "\n",
    "assert len(info_pairs) == len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, vectors = [], []\n",
    "\n",
    "for i, example in enumerate(examples):\n",
    "    example = example.lower().strip()\n",
    "    tokenized_text, indexed_tokens, segments_ids = tokenize(example)\n",
    "    embeddings = get_representation(indexed_tokens, segments_ids)\n",
    "\n",
    "    for wv in embeddings[0]:\n",
    "        indices.append(i)\n",
    "        vectors.append(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torch.stack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\"I try to finish the project.\"]\n",
    "\n",
    "sent = sents[0].lower().strip()\n",
    "tokenized_text, indexed_tokens, segments_ids = tokenize(sent)\n",
    "embeddings = get_representation(indexed_tokens, segments_ids)\n",
    "\n",
    "index = tokenized_text.index('try')\n",
    "target_emb = embeddings[0][index]\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(target_emb.unsqueeze(0), vectors)\n",
    "    \n",
    "matches = output > 0.7\n",
    "if any(matches):\n",
    "    for i, is_match in enumerate(matches):\n",
    "        if is_match:\n",
    "            example_idx = indices[i]\n",
    "            print(examples[example_idx])\n",
    "            print(info_pairs[example_idx])\n",
    "            print(output[i:i+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import numpy as np\n",
    "\n",
    "word2Idx = {}\n",
    "embeddings = []\n",
    "\n",
    "embeddingsIn = gzip.open('../embeddings/word2vec.txt.gz', \"rt\", encoding='utf8')\n",
    "embeddingsDimension = None\n",
    "\n",
    "for line in embeddingsIn:\n",
    "    split = line.rstrip().split(\" \")\n",
    "    word = split[0]\n",
    "\n",
    "    if embeddingsDimension == None:\n",
    "        embeddingsDimension = len(split) - 1\n",
    "\n",
    "    if (len(\n",
    "            split) - 1) != embeddingsDimension:  # Assure that all lines in the embeddings file are of the same length\n",
    "        print(\"ERROR: A line in the embeddings file had more or less  dimensions than expected. Skip token.\")\n",
    "        continue\n",
    "\n",
    "    if len(word2Idx) == 0:  # Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(embeddingsDimension)\n",
    "        embeddings.append(vector)\n",
    "\n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, embeddingsDimension)  # Alternativ -sqrt(3/dim) ... sqrt(3/dim)\n",
    "        embeddings.append(vector)\n",
    "\n",
    "    vector = np.array([float(num) for num in split[1:]])\n",
    "\n",
    "    if word not in word2Idx:\n",
    "        embeddings.append(vector)\n",
    "        word2Idx[word] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Idx['apple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.Dictionary import Dictionary\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# dictionary = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from collections import defaultdict\n",
    "from utils.config import level_table\n",
    "\n",
    "\n",
    "# ### Just lookup dictionary directly (ignore POS)\n",
    "\n",
    "class Dictionary:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab_dict = defaultdict(lambda: defaultdict(lambda: []))\n",
    "        self.vocab_to_pos = defaultdict(lambda: [])\n",
    "        self.pos_to_vocabs = defaultdict(lambda: [])\n",
    "        self.word2vec =  KeyedVectors.load_word2vec_format(\"/atom/word_vectors/GoogleNews-vectors-negative300.bin\", binary=True)  # C bin format\n",
    "        \n",
    "        for line in open('./data/dict.slim.txt', 'r', encoding='utf8'):\n",
    "            vocab, level, poss, gw, href = line.split('\\t')\n",
    "            \n",
    "            for pos in poss.replace(\";\", \",\").split(','):\n",
    "                pos = pos.strip().lower()\n",
    "                self.vocab_dict[vocab][pos] = level\n",
    "                self.vocab_to_pos[vocab].append(pos)\n",
    "                self.pos_to_vocabs[pos].append(vocab)\n",
    "            \n",
    "\n",
    "    def lookup(self, vocab):\n",
    "        if vocab not in self.vocab_dict: return None\n",
    "        \n",
    "        poss = self.vocab_to_pos[vocab]\n",
    "        return [self.vocab_dict[vocab][pos] for pos in poss]\n",
    "    \n",
    "        \n",
    "    def recommend(self, vocab):\n",
    "        if vocab not in self.vocab_dict: return None\n",
    "        \n",
    "        poss = self.vocab_to_pos[vocab]\n",
    "        \n",
    "        candidates = defaultdict(lambda: [])\n",
    "        for pos in poss:\n",
    "            for word in self.pos_to_vocabs[pos]:\n",
    "                candidates[word].extend((pos, self.vocab_dict[word][pos]))\n",
    "        for word in candidates:\n",
    "            candidates[word] = set(candidates[word])\n",
    "            \n",
    "        for sim, score in self.word2vec.similar_by_word(vocab, topn=100):\n",
    "            if sim in candidates:\n",
    "                print(sim)\n",
    "                print(candidates[sim])\n",
    "\n",
    "#         return self.vocab_level[vocab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want\n",
      "{'verb', 'A1'}\n",
      "attempt\n",
      "{'verb', 'B2', 'B1', 'noun'}\n",
      "let\n",
      "{'verb', 'B1'}\n",
      "do\n",
      "{'verb', 'A2'}\n",
      "help\n",
      "{'verb', 'B2', 'noun'}\n",
      "need\n",
      "{'verb', 'B2', 'B1', 'noun'}\n",
      "seek\n",
      "{'verb', 'C2'}\n",
      "hope\n",
      "{'verb', 'A2', 'B1', 'noun'}\n",
      "strive\n",
      "{'verb', 'C2'}\n",
      "can\n",
      "{'A2', 'noun'}\n",
      "chance\n",
      "{'B2', 'noun'}\n",
      "aim\n",
      "{'B1', 'noun'}\n",
      "intend\n",
      "{'verb', 'B1'}\n",
      "think\n",
      "{'verb', 'B2'}\n",
      "urge\n",
      "{'verb', 'C2', 'noun'}\n"
     ]
    }
   ],
   "source": [
    "dictionary.lookup('try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
